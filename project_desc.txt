Cost-Control Smart Model Router
Category: Infrastructure / AI Systems Engineering
üéØ Objective
Create a cost-optimized model routing system that intelligently decides which LLM (Phi-3, Llama 3, GPT-4o) to use based on the difficulty of incoming prompts ‚Äî minimizing API cost without compromising accuracy.
üß© System Architecture
Frontend: None (internal API)Backend: FastAPI endpointAI Core: Fine-tuned Phi-3 mini classifier modelData/Infra: PostgreSQL for request logs, cost tracking, and performance metrics.
üõ†Ô∏è Tech Stack
Python, FastAPI, HuggingFace Transformers, Phi-3 Mini, Llama 3, GPT-4o, PostgreSQL, Docker
üöÄ Implementation Steps
- Collect and label prompts by difficulty (simple, medium, complex).
- Fine-tune a Phi-3-mini model to classify prompt complexity.
- Build a FastAPI endpoint /generate that classifies and routes prompts to different models.
- Store each request and token cost in PostgreSQL for analytics.
- Visualize performance metrics through a dashboard.
üìà Expected Outcomes
Achieves 30‚Äì40% cost reduction while maintaining task accuracy and model efficiency.

ChatGPT said:
Thought for 4 seconds

Awesome project. Here‚Äôs a no-fluff, start-building blueprint with code scaffolds, DB schema, routing logic, training steps, and a Docker setup you can drop into a repo.

1) High-level plan (what you‚Äôll have by the end)

A FastAPI service exposing POST /generate.

A Phi-3-mini classifier fine-tuned to tag prompts: simple / medium / complex (+ a confidence score).

A router that selects Phi-3, Llama-3, or GPT-4o based on difficulty + guardrails.

PostgreSQL with tables for requests, costs, and model metrics.

Background analytics job to compute win rates, accuracy, and cost deltas.

Grafana/Metabase (pick one) hooked to Postgres for the dashboard.

2) Architecture (thin and pragmatic)
Client/Internal -> FastAPI /generate
                    |  
                    |---> DifficultyClassifier (Phi-3-mini, HF)
                    |
                    +---> Router(policy) ----> ModelClient( Phi-3 | Llama-3 | GPT-4o )
                    |
                    +---> PG Logger (requests, tokens, cost, latency, label_if_available)
                                   \
                                    \--> Metrics job (accuracy, cost/1k, winrate)
Dashboard (Grafana/Metabase)  <-----/

3) Data & labeling (fast path you can actually do)

Sources

Your team‚Äôs past prompts + outcomes.

Public prompt sets or synthetic prompts (programmatically create ‚Äúsimple/medium/complex‚Äù via templates: short Q&A vs. chain-of-thought math vs. multi-step coding/spec).

Weak labeling heuristics (bootstrap):

simple: length < 120 chars, no code blocks, no ‚Äúexplain step-by-step‚Äù, no math keywords.

medium: 120‚Äì400 chars OR has code/API/library keywords OR requires multi-step.

complex: > 400 chars OR contains ‚Äúprove‚Äù, ‚Äúdesign architecture‚Äù, ‚Äúoptimize‚Äù, ‚Äúwrite production code‚Äù, legal/medical disclaimers, multi-document context.

Then human-correct ~1‚Äì2k rows to seed the classifier. Keep difficulty, notes, and (optional) gold_route if you want supervised policy learning later.

Dataset format (JSONL)

{"text":"Explain KNN vs. K-means...", "label":"simple"}
{"text":"Given this API spec... write production-ready client...", "label":"complex"}

4) Fine-tune the Phi-3 mini classifier

Minimal HuggingFace training loop (single-label classification). Keep it simple; tune later.

# train_classifier.py
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
from sklearn.metrics import f1_score, accuracy_score

MODEL_NAME = "microsoft/phi-3-mini-4k-instruct"  # or compatible small variant
LABELS = ["simple", "medium", "complex"]
id2label = {i:l for i,l in enumerate(LABELS)}
label2id = {l:i for i,l in enumerate(LABELS)}

ds = load_dataset("json", data_files={"train":"data/train.jsonl", "val":"data/val.jsonl"})

tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, padding_side="left", truncation_side="left")
def encode(batch):
    out = tok(batch["text"], truncation=True, padding=True, max_length=512)
    out["labels"] = [label2id[x] for x in batch["label"]]
    return out

ds = ds.map(encode, batched=True).remove_columns(["text","label"])

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME, num_labels=len(LABELS), id2label=id2label, label2id=label2id
)

def metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "acc": accuracy_score(labels, preds),
        "f1_macro": f1_score(labels, preds, average="macro")
    }

args = TrainingArguments(
    output_dir="out",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    learning_rate=5e-5,
    num_train_epochs=3,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    logging_steps=50,
    fp16=True
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=ds["train"],
    eval_dataset=ds["val"],
    tokenizer=tok,
    compute_metrics=metrics
)

trainer.train()
trainer.save_model("artifacts/phi3-difficulty-classifier")
tok.save_pretrained("artifacts/phi3-difficulty-classifier")


Targets: acc ‚â• 0.8, f1_macro ‚â• 0.8 on held-out. If lower, enrich data and fix heuristics.

5) Routing policy (works well in practice)

Use difficulty + confidence + guardrails:

If simple and conf ‚â• 0.7 ‚Üí Phi-3 (cheap).

If medium and conf ‚â• 0.6 ‚Üí Llama-3.

If complex or conf < thresholds ‚Üí GPT-4o.

Overrides: presence of code blocks, explicit ‚Äúproduction‚Äù, ‚Äúlegal/medical‚Äù, ‚Äúarchitect‚Äù, ‚Äúoptimize for latency‚Äù, or customer_tier=Premium ‚Üí bump one level.

Max tokens and context length: if input + expected output > small model limits ‚Üí escalate.

Retry: if lower-tier returns low quality (fast heuristic: too short, refusal, or toxicity/PII guard), escalate once.

6) Cost model (keep in Postgres)

Maintain a cost_rates table per model and version:

model_name, input_cost_per_1k, output_cost_per_1k, effective_date.
Update manually when provider prices change.

7) Postgres schema (SQL)
CREATE TABLE IF NOT EXISTS requests (
  id BIGSERIAL PRIMARY KEY,
  ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  user_id TEXT,
  prompt TEXT NOT NULL,
  difficulty TEXT,          -- predicted label
  difficulty_conf NUMERIC,  -- 0..1
  routed_model TEXT NOT NULL,
  provider_request_id TEXT,
  input_tokens INT,
  output_tokens INT,
  latency_ms INT,
  cost_input_us INT,        -- store in micros to avoid floats
  cost_output_us INT,
  total_cost_us INT,
  status TEXT,              -- success|fail|escalated|fallback
  response_preview TEXT
);

CREATE TABLE IF NOT EXISTS cost_rates (
  id SERIAL PRIMARY KEY,
  model_name TEXT NOT NULL,
  input_cost_per_1k_us INT NOT NULL,
  output_cost_per_1k_us INT NOT NULL,
  effective_date DATE NOT NULL
);

CREATE TABLE IF NOT EXISTS model_eval (
  id BIGSERIAL PRIMARY KEY,
  ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),
  request_id BIGINT REFERENCES requests(id),
  human_score INT,          -- 1..5
  pass_fail BOOLEAN,
  notes TEXT
);

CREATE INDEX ON requests (ts);
CREATE INDEX ON requests (routed_model);
CREATE INDEX ON requests (difficulty);

8) FastAPI service (drop-in scaffold)
# app/main.py
import time
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import psycopg2
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import os

LABELS = ["simple", "medium", "complex"]

# --- load classifier
clf_name = os.getenv("CLASSIFIER_PATH", "artifacts/phi3-difficulty-classifier")
tok = AutoTokenizer.from_pretrained(clf_name)
clf = AutoModelForSequenceClassification.from_pretrained(clf_name)
clf.eval()

def predict_difficulty(text: str):
    with torch.no_grad():
        inputs = tok(text, return_tensors="pt", truncation=True, max_length=512)
        logits = clf(**inputs).logits
        probs = torch.softmax(logits, dim=-1)[0].tolist()
        idx = int(torch.argmax(logits, dim=-1))
        return LABELS[idx], probs[idx], probs  # (label, conf, all_probs)

# --- dummy token counter (swap with provider-specific)
def token_count(s: str) -> int:
    # rough count: ~4 chars/token heuristic
    return max(1, len(s) // 4)

def pick_model(label: str, conf: float, prompt: str) -> str:
    guard_escalate = any(k in prompt.lower() for k in [
        "production", "legal", "medical", "architecture", "optimize", "compliance"
    ])
    if guard_escalate:
        if label == "simple":
            return "llama-3"
        elif label == "medium":
            return "gpt-4o"
        return "gpt-4o"

    if label == "simple" and conf >= 0.7:
        return "phi-3"
    if label == "medium" and conf >= 0.6:
        return "llama-3"
    return "gpt-4o"

def price_us(model: str, in_tokens: int, out_tokens: int) -> int:
    # Stub constants; replace by joining cost_rates in DB
    rates = {
        "phi-3":  (50, 100),     # micros per 1k (example)
        "llama-3":(150, 300),
        "gpt-4o": (500, 1500)
    }
    ri, ro = rates[model]
    return (in_tokens * ri // 1000) + (out_tokens * ro // 1000)

# --- provider stubs (replace with real clients)
async def call_phi3(prompt):   return "phi3: "  + prompt[:400]
async def call_llama3(prompt): return "llama3: "+ prompt[:400]
async def call_gpt4o(prompt):  return "gpt4o: " + prompt[:400]

MODEL_CALL = {
    "phi-3": call_phi3,
    "llama-3": call_llama3,
    "gpt-4o": call_gpt4o
}

# --- DB
def get_conn():
    return psycopg2.connect(os.getenv("PG_DSN", "postgresql://postgres:postgres@db:5432/router"))

app = FastAPI()

class GenerateIn(BaseModel):
    user_id: str | None = None
    prompt: str

class GenerateOut(BaseModel):
    model: str
    difficulty: str
    confidence: float
    response: str
    input_tokens: int
    output_tokens: int
    total_cost_us: int
    latency_ms: int

@app.post("/generate", response_model=GenerateOut)
async def generate(inp: GenerateIn):
    t0 = time.time()
    label, conf, _ = predict_difficulty(inp.prompt)
    model = pick_model(label, conf, inp.prompt)
    in_toks = token_count(inp.prompt)

    try:
        response = await MODEL_CALL[model](inp.prompt)
    except Exception as e:
        # fallback once
        if model != "gpt-4o":
            model = "gpt-4o"
            response = await MODEL_CALL[model](inp.prompt)
        else:
            raise HTTPException(502, f"provider error: {e}")

    out_toks = token_count(response)
    cost = price_us(model, in_toks, out_toks)
    latency = int((time.time() - t0) * 1000)

    # log
    try:
        with get_conn() as c, c.cursor() as cur:
            cur.execute("""
                INSERT INTO requests(user_id, prompt, difficulty, difficulty_conf,
                    routed_model, input_tokens, output_tokens, latency_ms,
                    cost_input_us, cost_output_us, total_cost_us, status, response_preview)
                VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,'success',%s)
                RETURNING id
            """, (
                inp.user_id, inp.prompt, label, conf, model,
                in_toks, out_toks, latency,
                0, 0, cost, response[:500]
            ))
    except Exception as e:
        # don't fail request if logging breaks
        print("DB log error:", e)

    return GenerateOut(
        model=model, difficulty=label, confidence=conf, response=response,
        input_tokens=in_toks, output_tokens=out_toks, total_cost_us=cost, latency_ms=latency
    )

9) Docker & Compose (runs API + Postgres + Adminer)
# docker-compose.yml
version: "3.9"
services:
  api:
    build: .
    ports: ["8000:8000"]
    environment:
      - CLASSIFIER_PATH=/models/phi3-clf
      - PG_DSN=postgresql://postgres:postgres@db:5432/router
    depends_on: [db]
    volumes:
      - ./artifacts/phi3-difficulty-classifier:/models/phi3-clf:ro

  db:
    image: postgres:16
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: router
    ports: ["5432:5432"]
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./sql:/docker-entrypoint-initdb.d:ro  # put the schema here

  adminer:
    image: adminer
    ports: ["8080:8080"]
    depends_on: [db]

volumes:
  pgdata:


Dockerfile

# Dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY app/ /app/app/
COPY requirements.txt /app/
RUN pip install --no-cache-dir -r requirements.txt
EXPOSE 8000
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]


requirements.txt

fastapi
uvicorn[standard]
transformers
torch
datasets
psycopg2-binary
scikit-learn


Put your schema SQL into ./sql/001_schema.sql so Postgres initializes it.

10) Evaluation & ‚Äú30‚Äì40% cost reduction‚Äù proof

Offline Harness

Sample ~1‚Äì2k historical prompts.

For each prompt, run all three models (frozen temperature & system prompt).

Ask human raters (or rubric) to grade responses (pass/fail + 1‚Äì5 score).

Compute Cost@Pass and Win Rate for:

Always-Phi3, Always-Llama3, Always-GPT4o, and Router.

Report: Cost savings = 1 - (Router mean cost / Baseline mean cost).
Quality guard: Router pass_rate ‚â• Baseline pass_rate - 1%.

Online

Randomly bucket 10% traffic to control (Always-Llama3) and 10% to Router.

Track total_cost_us/request, pass_rate (via thumbs or quick rubric), latency_ms.

11) Dashboard queries (starter)

Avg cost per request/day

SELECT date_trunc('day', ts) d, AVG(total_cost_us)/1000000 AS cost_usd
FROM requests
GROUP BY 1 ORDER BY 1;


Model mix

SELECT routed_model, COUNT(*) FROM requests GROUP BY 1 ORDER BY 2 DESC;


Latency distribution per model

SELECT routed_model, PERCENTILE_DISC(0.5) WITHIN GROUP (ORDER BY latency_ms) AS p50,
       PERCENTILE_DISC(0.95) WITHIN GROUP (ORDER BY latency_ms) AS p95
FROM requests GROUP BY 1;


Quality vs. Cost (needs model_eval)

SELECT date_trunc('day', r.ts) d,
       AVG(me.pass_fail::int) pass_rate,
       AVG(r.total_cost_us)/1e6 cost_usd
FROM requests r
JOIN model_eval me ON me.request_id = r.id
GROUP BY 1 ORDER BY 1;

12) Practical guardrails & ops

Prompt precheck: refuse unsafe, PII, and disallowed content before routing.

Rate-limit backoff: exponential backoff + provider fallback.

Timeouts: per model 20‚Äì40s; if timeout on lower tier, escalate once.

Content length: if input > small-model context, auto-escalate.

Versioning: store provider_request_id and model version (e.g., gpt-4o-2025-10-01).

13) Milestones (get to value fast)

Week 1

Collect data, heuristics labeling, train first classifier, wire FastAPI + PG, log requests.
Week 2

Plug real providers, implement policy, ship MVP, start offline eval.
Week 3

Close loop on quality (escalation rules), add dashboard, run online A/B, publish savings.

What to do next (copy/paste tasks)

Create repo with folders: app/, artifacts/, data/, sql/.

Drop the schema into sql/001_schema.sql.

Train the classifier with train_classifier.py and save to artifacts/phi3-difficulty-classifier/.

Build & run docker compose up --build.

Hit POST /generate:

curl -X POST localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"user_id":"u1","prompt":"Write a 2-sentence summary of KNN vs K-means."}'


Wire real model clients + real token counting + real prices in price_us().